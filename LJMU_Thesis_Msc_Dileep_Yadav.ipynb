{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DILEEP-YADAV/LJMU_RESEARCH_PROJECT/blob/main/LJMU_Thesis_Msc_Dileep_Yadav.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "                                      Liverpool John Moores University\n",
        "                                   Faculty of Engineering and Technology\n",
        "\n",
        "\n",
        "               “Neurological Disorders Detection from Speech Emotions Using  Convolutional Neural Network”\n",
        "\n",
        "                                                      By\n",
        "                                                 Dileep Yadav\n",
        "\n",
        "                                    A mid thesis submitted for the degree of\n",
        "                            MSc Artificial Intelligence (Machine Learning October 2020\n",
        "\n",
        "\n",
        "                                               THESIS COMMITTEE\n",
        "\n",
        "                                   Thesis supervisor: Sujith Viswanathan\n",
        "                           Platform Partner: upGrad Education Private Limited\n",
        "                     Committee member: A Faculty of Engineering and Technology, LJMU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJBs_flRovLc"
      },
      "source": [
        "## **Getting started**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJr_9dXGpJ05"
      },
      "source": [
        "import scipy.io as sio\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import pickle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gE-Ez1qtyIA"
      },
      "source": [
        "np.random.seed(0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4HZx7Gndbrh"
      },
      "source": [
        "def data_1Dto2D(data, Y=9, X=9):\n",
        "    data_2D = np.zeros([Y, X])\n",
        "    data_2D[0] = (0,  \t   \t0, \t        0,          data[0],    0,          data[16], \t0,  \t    0, \t        0       )\n",
        "    data_2D[1] = (0,  \t   \t0,          0,          data[1],    0,          data[17],   0,          0,          0       )\n",
        "    data_2D[2] = (data[3],  0,          data[2],    0,          data[18],   0,          data[19],   0,          data[20])\n",
        "    data_2D[3] = (0,        data[4],    0,          data[5],    0,          data[22],   0,          data[21],   0       )\n",
        "    data_2D[4] = (data[7],  0,          data[6],    0,          data[23],   0,          data[24],   0,          data[25])\n",
        "    data_2D[5] = (0,        data[8],    0,          data[9],    0,          data[27],   0,          data[26],   0       )\n",
        "    data_2D[6] = (data[11], 0,          data[10],   0,          data[15],   0,          data[28],   0,          data[29])\n",
        "    data_2D[7] = (0,        0,          0,          data[12],   0,          data[30],   0,          0,          0       )\n",
        "    data_2D[8] = (0,        0,          0,          data[13],   data[14],   data[31],   0,          0,          0       )\n",
        "    # return shape:9*9\n",
        "    return data_2D\n",
        "\n",
        "    "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyNvDH9QPiZo"
      },
      "source": [
        "def norm_dataset(dataset_1D):\n",
        "    norm_dataset_1D = np.zeros([dataset_1D.shape[0], 32])\n",
        "    for i in range(dataset_1D.shape[0]):\n",
        "        norm_dataset_1D[i] = feature_normalize(dataset_1D[i])\n",
        "    # return shape: m*32\n",
        "    return norm_dataset_1D"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UnPhoDAP5HB"
      },
      "source": [
        "def feature_normalize(data):\n",
        "    mean = data[data.nonzero()].mean()\n",
        "    sigma = data[data. nonzero ()].std()\n",
        "    data_normalized = data\n",
        "    data_normalized[data_normalized.nonzero()] = (data_normalized[data_normalized.nonzero()] - mean)/sigma\n",
        "    # return shape: 9*9\n",
        "    return data_normalized"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr0Eooj_P_qR"
      },
      "source": [
        "def dataset_1Dto2D(dataset_1D):\n",
        "    dataset_2D = np.zeros([dataset_1D.shape[0],9,9])\n",
        "    for i in range(dataset_1D.shape[0]):\n",
        "        dataset_2D[i] = data_1Dto2D(dataset_1D[i])\n",
        "    # return shape: m*9*9\n",
        "    return dataset_2D"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0Q9zBp1QFYd"
      },
      "source": [
        "def norm_dataset_1Dto2D(dataset_1D):\n",
        "    norm_dataset_2D = np.zeros([dataset_1D.shape[0], 9, 9])\n",
        "    for i in range(dataset_1D.shape[0]):\n",
        "        norm_dataset_2D[i] = feature_normalize( data_1Dto2D(dataset_1D[i]))\n",
        "    # return shape: m*9*9\n",
        "    return norm_dataset_2D"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4GhHZzhQKSr"
      },
      "source": [
        "def windows(data, size):\n",
        "    start = 0\n",
        "    while ((start+size) < data.shape[0]):\n",
        "        yield int(start), int(start + size)\n",
        "        start += size"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2660NEjQSXN"
      },
      "source": [
        "def segment_signal_without_transition(data,label,label_index,window_size):\n",
        "    # get data file name and label file name\n",
        "    for (start, end) in windows(data, window_size):\n",
        "        # print(data.shape)\n",
        "        if((len(data[start:end]) == window_size)):\n",
        "            if(start == 0):\n",
        "                segments = data[start:end]\n",
        "                segments = np.vstack([segments, data[start:end]])\n",
        "\n",
        "                labels = np.array(label[label_index])\n",
        "                labels = np.append(labels, np.array(label[label_index]))\n",
        "            else:\n",
        "                segments = np.vstack([segments, data[start:end]])\n",
        "                labels = np.append(labels, np.array(label[label_index])) # labels = np.append(labels, stats.mode(label[start:end])[0][0])\n",
        "    return segments, labels"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Japn6g0VQV33"
      },
      "source": [
        "def apply_mixup(dataset_file,window_size,label,yes_or_not): # initial empty label arrays\n",
        "    print(\"Processing\",dataset_file,\"..........\")\n",
        "    data_file_in = sio.loadmat(dataset_file)\n",
        "    data_in = data_file_in[\"data\"].transpose(0,2,1)\n",
        "    #0 valence, 1 arousal, 2 dominance, 3 liking\n",
        "    if label==\"arousal\":\n",
        "        label=1\n",
        "    elif label==\"valence\":\n",
        "        label=0\n",
        "    label_in= data_file_in[\"labels\"][:,label]>5\n",
        "    label_inter\t= np.empty([0]) # initial empty data arrays\n",
        "    data_inter_cnn\t= np.empty([0,window_size, 9, 9])\n",
        "    data_inter_rnn\t= np.empty([0, window_size, 32])\n",
        "    trials = data_in.shape[0]\n",
        "\n",
        "    # Data pre-processing\n",
        "    for trial in range(0,trials):\n",
        "        if yes_or_not==\"yes\":\n",
        "            base_signal = (data_in[trial,0:128,0:32]+data_in[trial,128:256,0:32]+data_in[trial,256:384,0:32])/3\n",
        "        else:\n",
        "            base_signal = 0\n",
        "        data = data_in[trial,384:8064,0:32]\n",
        "        # compute the deviation between baseline signals and experimental signals\n",
        "        for i in range(0,60):\n",
        "            data[i*128:(i+1)*128,0:32]=data[i*128:(i+1)*128,0:32]-base_signal\n",
        "        label_index = trial\n",
        "        #read data and label\n",
        "        data = norm_dataset(data)\n",
        "        data, label = segment_signal_without_transition(data, label_in,label_index,window_size)\n",
        "        # cnn data process\n",
        "        data_cnn    = dataset_1Dto2D(data)\n",
        "        data_cnn    = data_cnn.reshape ( int(data_cnn.shape[0]/window_size), window_size, 9, 9)\n",
        "        # rnn data process\n",
        "        data_rnn    = data. reshape(int(data.shape[0]/window_size), window_size, 32)\n",
        "        # append new data and label\n",
        "        data_inter_cnn  = np.vstack([data_inter_cnn, data_cnn])\n",
        "        data_inter_rnn  = np.vstack([data_inter_rnn, data_rnn])\n",
        "        label_inter = np.append(label_inter, label)\n",
        "    '''\n",
        "    print(\"total cnn size:\", data_inter_cnn.shape)\n",
        "    print(\"total rnn size:\", data_inter_rnn.shape)\n",
        "    print(\"total label size:\", label_inter.shape)\n",
        "    '''\n",
        "    # shuffle data\n",
        "    index = np.array(range(0, len(label_inter)))\n",
        "    np.random.shuffle( index)\n",
        "    shuffled_data_cnn\t= data_inter_cnn[index]\n",
        "    shuffled_data_rnn\t= data_inter_rnn[index]\n",
        "    shuffled_label \t= label_inter[index]\n",
        "    return shuffled_data_cnn ,shuffled_data_rnn,shuffled_label,record\n",
        "\n",
        "if __name__ == '__main__' :\n",
        "    begin = time.time()\n",
        "    print(\"time begin:\",time.localtime())\n",
        "    dataset_dir\t\t=   \"/home/data_preprocessed_matlab/\"\n",
        "    window_size\t\t=\t128\n",
        "    output_dir\t\t=   \"./deap_shuffled_data/\"\n",
        "    label_class     =   sys.argv[1]     # arousal/valence\n",
        "    suffix          =   sys.argv[2]     # yes/no (using baseline signals or not)\n",
        "    # get directory name for one subject\n",
        "    record_list = [task for task in os.listdir(dataset_dir) if os.path.isfile(os.path.join(dataset_dir,task))]\n",
        "    output_dir = output_dir+suffix+\"_\"+label_class+\"/\"\n",
        "    if os.path.isdir(output_dir)==False:\n",
        "        os.makedirs(output_dir)\n",
        "    # print(record_list)\n",
        "\n",
        "    for record in record_list:\n",
        "        file = os.path.join(dataset_dir,record)\n",
        "        shuffled_cnn_data,shuffled_rnn_data,shuffled_label,record = apply_mixup(file, window_size,label_class,suffix)\n",
        "        output_data_cnn = output_dir+record+\"_win_\"+str(window_size)+\"_cnn_dataset.pkl\"\n",
        "        output_data_rnn = output_dir+record+\"_win_\"+str(window_size)+\"_rnn_dataset.pkl\"\n",
        "        output_label= output_dir+record+\"_win_\"+str(window_size)+\"_labels.pkl\"\n",
        "\n",
        "        with open(output_data_cnn, \"wb\") as fp:\n",
        "            pickle.dump( shuffled_cnn_data,fp, protocol=4)\n",
        "        with open( output_data_rnn, \"wb\") as fp:\n",
        "            pickle.dump(shuffled_rnn_data, fp, protocol=4)\n",
        "        with open(output_label, \"wb\") as fp:\n",
        "            pickle.dump(shuffled_label, fp)\n",
        "        end = time.time()\n",
        "        print(\"end time:\",time.localtime())\n",
        "        print(\"time consuming:\",(end-begin))\n",
        "        \n",
        "        # break"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}